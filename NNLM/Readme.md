
##神经概率语言网络NNLM
网络结构有四层：输入层、投影层、隐藏层和输出层，具体参数为：
1. 词库大小（假设8w个词）
2. 转化的词向量大小（假设300维度）
3. 输入层神经元数（即词的滑动窗口容量n-1，n = 4）
4. 隐藏层神经元数量（假设100个，自己设定）
5. 输出层神经元（对应词容量，8w）
6. 由输入层到投影层的矩阵C（存在矩阵 C（一个 |V|×m 的矩阵）中。其中 |V| 表示词表的大小（语料中的总词数），m 表示词向量的维度。w 到 C(w) 的转化就是从矩阵中取出一行。8w*300）
7. 从投影层到隐层的权值矩阵W和偏置矩阵p
8. 从隐层到输出层的权值矩阵U和偏置矩阵q
![2](https://github.com/GgKAkaNo/NLP_tutorial/blob/master/NNLM/png/2.png)

### 步骤
1. 每次从语料库的任意一个词w，取其前面的3个词，转为one-hot编码
2. 从输入到映射层就是one-hot向量*矩阵C，这里one-hot编码会根据1的位置去对应C矩阵，去抽取对应位置的300维的词向量，是将 C(w_{t−n+1}),…,C(w_{t−2}),C(w{t−1})这 n−1 个向量首尾相接拼起来，形成一个 (n−1)m 维的向量，下面记为 x。
3. tanh为双曲正切函数，用来隐藏层的激活函数。$z_w=\tanh(Wx+p)$
4. y_w=Uz_w+q
5. 计算交叉熵损失函数值，以梯度下降方式进行反向传播，更新参数
6. 最后得到$y=(y_{w,1},...,y_{w,N})^T$,N为词汇量

![1](https://github.com/GgKAkaNo/NLP_tutorial/blob/master/NNLM/png/1.png)
